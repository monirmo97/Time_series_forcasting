{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "\n",
        "class ETTH1Dataset(Dataset):\n",
        "    def __init__(self, data, seq_len, label_len, pred_len, transform=None):\n",
        "        self.data = data\n",
        "        self.seq_len = seq_len\n",
        "        self.label_len = label_len\n",
        "        self.pred_len = pred_len\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.seq_len - self.pred_len + 1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq_x = self.data[idx:idx + self.seq_len, :-1]\n",
        "        seq_y = self.data[idx + self.seq_len:idx + self.seq_len + self.pred_len, -1]\n",
        "\n",
        "        if self.transform:\n",
        "            seq_x = self.transform(seq_x)\n",
        "            seq_y = self.transform(seq_y)\n",
        "\n",
        "        return seq_x, seq_y\n",
        "\n",
        "def load_data(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    df.set_index('date', inplace=True)\n",
        "    return df\n",
        "\n",
        "def preprocess_data(df):\n",
        "    scaler_features = StandardScaler()\n",
        "    scaler_target = StandardScaler()\n",
        "    df[['HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL']] = scaler_features.fit_transform(df[['HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL']])\n",
        "    df['OT'] = scaler_target.fit_transform(df[['OT']])\n",
        "    return df, scaler_features, scaler_target\n",
        "\n",
        "def split_data(df, seq_len, label_len, pred_len):\n",
        "    data = df[['HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL', 'OT']].values\n",
        "    train_data, test_data = train_test_split(data, test_size=0.2, shuffle=False)\n",
        "    train_data, val_data = train_test_split(train_data, test_size=0.25, shuffle=False)\n",
        "\n",
        "    train_dataset = ETTH1Dataset(train_data, seq_len, label_len, pred_len)\n",
        "    val_dataset = ETTH1Dataset(val_data, seq_len, label_len, pred_len)\n",
        "    test_dataset = ETTH1Dataset(test_data, seq_len, label_len, pred_len)\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_dim, model_dim, output_dim, num_heads, num_encoder_layers, num_decoder_layers, dropout=0.1):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.model_dim = model_dim\n",
        "\n",
        "        self.encoder_embedding = nn.Linear(input_dim, model_dim)\n",
        "        self.decoder_embedding = nn.Linear(output_dim, model_dim)\n",
        "        self.positional_encoding = nn.Parameter(torch.zeros(1, 1000, model_dim))\n",
        "\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads, batch_first=True, dropout=dropout)\n",
        "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_encoder_layers)\n",
        "\n",
        "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=model_dim, nhead=num_heads, batch_first=True, dropout=dropout)\n",
        "        self.decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=num_decoder_layers)\n",
        "\n",
        "        self.output_layer = nn.Linear(model_dim, output_dim)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src = self.encoder_embedding(src) + self.positional_encoding[:, :src.size(1), :]\n",
        "        tgt = self.decoder_embedding(tgt) + self.positional_encoding[:, :tgt.size(1), :]\n",
        "\n",
        "        memory = self.encoder(src)\n",
        "        output = self.decoder(tgt, memory)\n",
        "\n",
        "        return self.output_layer(output)\n",
        "\n",
        "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=50):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for batch in train_loader:\n",
        "            src, tgt = batch\n",
        "            src, tgt = src.to(device).float(), tgt.to(device).float().unsqueeze(-1)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(src, tgt)\n",
        "            loss = criterion(output, tgt)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * src.size(0)\n",
        "\n",
        "        val_loss = 0.0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                src, tgt = batch\n",
        "                src, tgt = src.to(device).float(), tgt.to(device).float().unsqueeze(-1)\n",
        "                output = model(src, tgt)\n",
        "                loss = criterion(output, tgt)\n",
        "                val_loss += loss.item() * src.size(0)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_loader.dataset)}, Val Loss: {val_loss/len(val_loader.dataset)}\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), 'best_transformer_model.pth')\n",
        "\n",
        "def evaluate_model(model, criterion, test_loader, scaler_target, start_idx=0, end_idx=100, save_path=\"predictions_transformer.png\"):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    test_loss = 0.0\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            src, tgt = batch\n",
        "            src, tgt = src.to(device).float(), tgt.to(device).float().unsqueeze(-1)\n",
        "            output = model(src, tgt)\n",
        "            loss = criterion(output, tgt)\n",
        "            test_loss += loss.item() * src.size(0)\n",
        "            predictions.append(output.cpu().numpy())\n",
        "            actuals.append(tgt.cpu().numpy())\n",
        "\n",
        "    predictions = np.concatenate(predictions)\n",
        "    actuals = np.concatenate(actuals)\n",
        "    print(f\"Test MAE: {test_loss/len(test_loader.dataset)}\")\n",
        "\n",
        "    r2_ot = r2_score(actuals.reshape(-1, 1), predictions.reshape(-1, 1))\n",
        "    print(f\"R2 OT: {r2_ot}\")\n",
        "\n",
        "    predictions = scaler_target.inverse_transform(predictions.reshape(-1, 1)).reshape(predictions.shape)\n",
        "    actuals = scaler_target.inverse_transform(actuals.reshape(-1, 1)).reshape(actuals.shape)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(actuals[start_idx:end_idx].flatten(), label='Actual')\n",
        "    plt.plot(predictions[start_idx:end_idx].flatten(), label='Predicted')\n",
        "    plt.title('Oil Temperature Prediction')\n",
        "    plt.xlabel('Time Step')\n",
        "    plt.ylabel('Oil Temperature')\n",
        "    plt.legend()\n",
        "    plt.savefig(save_path)\n",
        "    plt.show()\n",
        "\n",
        "def main():\n",
        "    file_path = '/content/drive/MyDrive/ETTh1.csv'\n",
        "    seq_len = 96\n",
        "    label_len = 48\n",
        "    pred_len = 96\n",
        "\n",
        "    df = load_data(file_path)\n",
        "    df, scaler_features, scaler_target = preprocess_data(df)\n",
        "    train_dataset, val_dataset, test_dataset = split_data(df, seq_len, label_len, pred_len)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "    input_dim = 6  # 6 features\n",
        "    model_dim = 512\n",
        "    output_dim = 1\n",
        "    num_heads = 4\n",
        "    num_encoder_layers = 1\n",
        "    num_decoder_layers = 1\n",
        "    dropout = 0.1\n",
        "\n",
        "    model = TransformerModel(input_dim, model_dim, output_dim, num_heads, num_encoder_layers, num_decoder_layers, dropout)\n",
        "    criterion = nn.L1Loss()\n",
        "    # criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=1)\n",
        "    evaluate_model(model, criterion, test_loader, scaler_target, start_idx=100, end_idx=120, save_path=\"predictions_transformer.png\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "id": "UuFw0NTYwKMw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}